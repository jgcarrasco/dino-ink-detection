{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jgcarrasco/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from utils import load_segment, visualize_cropper, compute_PCA, visualize_PCA\n",
    "\n",
    "# Load the model\n",
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14_reg').cuda()\n",
    "model.eval()\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config\n",
    "\n",
    "Below you can change several parameters. I recommend you to play with the number of PCA components and the crop size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_DEPTH = False # Whether to compute PCA across al layers, or layerwise. I obtained better results with layerwise (i.e. False)\n",
    "N_COMPONENTS = 5 # Number of PCA components \n",
    "CROP_SIZE = 600 # Size of the crop that we want to compute PCA on. The crop size will be rounded to the smallest multiple of the patch size\n",
    "PATCH_SIZE = 14 # patch size that the DINO model uses, do not change!\n",
    "CROP_SIZE = (CROP_SIZE // PATCH_SIZE) * PATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to read the data, which has to be stored in the following format:\n",
    "\n",
    "```\n",
    "segments/\n",
    "    segment_id/\n",
    "        layers/\n",
    "            00.tif\n",
    "            ...\n",
    "            64.tif\n",
    "        segment_id_inklabels.png\n",
    "        segment_id_mask.png\n",
    "```\n",
    "\n",
    "**NOTE:** This implementation is a really inefficient prototype! Try to load smaller segments first, below I provide different suggestions. For now, we will just work with the segment containing Casey's pi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segment_id = \"20230509182749\" # casey pi's segment\n",
    "segment_id = \"20230519195952\" # another small segment\n",
    "#segment_id = \"20231210121321\" # small segment with lots of predicted letters. This starts to get larger and might not work in colab\n",
    "\n",
    "# load the segment\n",
    "tif, inklabels = load_segment(segment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below to visualize the segment. You can click on the segment to move the red square. Once that you have selected a region of your interest, press enter to crop it and keep following the notebook. Try to crop the whole region that was predicted as a pi!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subimage saved!\n"
     ]
    }
   ],
   "source": [
    "result = visualize_cropper(tif, inklabels, crop_size=CROP_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the PCA vectors on the cropped region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_tif = result[\"cropped_tif\"]\n",
    "cropped_inklabels = result[\"cropped_inklabels\"]\n",
    "y_pca = compute_PCA(cropped_tif, model, PCA_DEPTH, n_components=N_COMPONENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we visualize the results. Try to localize hints of the letter. If you cannot see anything, use the slider to move through the different layers, ensure that you have cropped the whole pi or try cropping a larger area. Now you can try visualizing other small segments/regions and try to discover letters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e0f37c4d3c14b1c8d9881b8d60b899c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=33, description='Slice Index', max=64), Output()), _dom_classes=('widgetâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_PCA(y_pca, cropped_tif, cropped_inklabels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vesuvius",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
